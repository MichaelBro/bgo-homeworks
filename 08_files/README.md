# Домашнее задание к занятию «3.1. Работа с файлами (io, bufio, ioutil)»

В качестве результата пришлите ссылки на ваши GitHub-проекты в личном кабинете студента на сайте [netology.ru](https://netology.ru).

Все задачи этого занятия нужно делать в **одном репозитории**.

**Важно**: если у вас что-то не получилось, то оформляйте Issue [по установленным правилам](../report-requirements.md).

**ВАЖНО**: НИ В КОЕМ СЛУЧАЕ НЕ ПОДСТАВЛЯЙТЕ ДАННЫЕ СВОИХ РЕАЛЬНЫХ КАРТ В КОД! Это очень частая "оплошность", когда разработчики случайно коммитят и заливают на GitHub "чувствительные" (sensitive) данные (ключи, логины, пароли, адреса и т.д.). Используйте генераторы вроде: https://www.freeformatter.com/credit-card-number-generator-validator.html

Если вы всё же "случайно" залили чувствительные данные на GitHub, то используйте [инструкцию по удалению данных](https://help.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository). Кроме того, как бы это печально не было, рекомендуем вам заблокировать карту и заказать в банке новую.

## Как сдавать задачи

1. Создайте на вашем компьютере Go-модуль (см. доп.видео к первой лекции)
1. Инициализируйте в нём пустой Git-репозиторий
1. Добавьте в него готовый файл [.gitignore](../.gitignore)
1. Добавьте в этот же каталог остальные необходимые файлы (убедитесь, что они аккуратно разложены по пакетам)
1. Удостоверьтесь, что вы правильно отформатировали файлы (см. раздел Форматирование из [первого ДЗ](../01_std))
1. Сделайте необходимые коммиты
1. Создайте публичный репозиторий на GitHub и свяжите свой локальный репозиторий с удалённым
1. Сделайте пуш (удостоверьтесь, что ваш код появился на GitHub)
1. Ссылку на ваш проект отправьте в личном кабинете на сайте [netology.ru](https://netology.ru)
1. Задачи, отмеченные, как необязательные, можно не сдавать, это не повлияет на получение зачета (в этом ДЗ все задачи являются обязательными)

## Задача №1 - Импорт

На лекции мы с вами смотрели на то, как можно экспортировать данные в формат CSV. Сейчас же поговорим о том, как их импортировать.

Задача импорта встречается реже и чаще является "административной" функцией (просто представьте, что будет, если мы случайно дадим пользователю возможность самостоятельно импортировать историю транзакций). Использует эту функцию оператор системы.

Вот здесь важная вещь: мы потихоньку приходим к тому, что мы начинаем разговаривать о системах, в которых существуют пользователи с разными ролями (а, соответственно, и полномочиями).

Итак, что же нам нужно, для того, чтобы сделать импорт:

1. Иметь возможность прочитать содержимое
1. Иметь возможность обработать его (если он структурирован в какой-то формат, например, CSV), говорят "распарсить"
1. Интерпретировать то, что мы распарсили в структуры данных приложения

Давайте начнём по-порядку.

### Чтение файла

Для чтения файла у нас существует способ в лоб:
```go
// открываем файл
file, err := os.Open("file.csv")
if err != nil {
    // TODO: handle error
}
// планируем закрытие
defer func(c io.Closer) {
    if cerr := c.Close(); cerr != nil {
        // TODO: handle error
    }
}(file)

// слайс для хранения всего содержимого
content := make([]byte, 0)
// буфер для чтения
buf := make([]byte, 4096)
for {
    // читаем в буфер            
    n, err := file.Read(buf)
    if err != nil {
        // io.EOF - ошибка, сигнализирующая о том, что дочитали данные до конца (файл закончился)
        if err != io.EOF {
            // TODO: handle error		
        }
        // "перекладываем" данные из буфера в слайс со всем содержимым
        content = append(content, buf[:n]...)
        break    
    }    
    content = append(content, buf[:n]...)
}
// TODO: file content in data - handle it
```

В целом, чтение похоже на запись, но есть ряд ключевых моментов, которые мы должны обсудить:

1. Мы используем буфер для чтения (т.е. читаем по 4096 байт) - так быстрее.
1. Файл может быть не кратен 4096, например, 5000 байт, тогда при последнем чтении в `n` будет 4, а в `err` специальная ошибка `io.EOF` (End Of File), сигнализирующая о том, что файл кончился, данных больше нет. Самое интересное, что с точки зрения нас, как программиста, это именно признак того, что мы дочитали до конца.
1. Мы записываем в буфер ровно столько, сколько прочитали (не байтом больше), а то получим "мусор" с предыдущего чтения
1. Мы используем `...` - чтобы передать не целиком слайс, а именно его элементы (как если бы их писали через запятую): `append(content, buf[0], buf[1], buf[2], ..., buf[n-1])`

Соответственно, если мы хотим читать именно CSV, то большая часть кода нам не нужна, поскольку `csv.Reader` всё сделает за нас:

```go
file, err := os.Open("file.csv")
if err != nil {
    // TODO: handle error
}
defer func(c io.Closer) {
    if cerr := c.Close(); cerr != nil {
        // TODO: handle error
    }
}(file)

reader := csv.NewReader(file)
records := make([][]string, 0)
for {
    record, err := reader.Read()
    if err != nil {
        if err != io.EOF { // та же самая логика
            // TODO: handle error
        }
        records = append(records, record)
        break
    }
    records = append(records, record)
}
```

Обратите внимание: в обоих случаях мы **всё** складывали в слайс в памяти. Это работает ровно до тех пор, пока у вас небольшие файлы. Если же ваши файлы будут размером в несколько сотен мегабайт и выше - это не лучшая идея (особенно если на вашем сервере не очень много оперативной памяти).

Если бы мы сразу обрабатывали прочитанные байты/записи - тогда эти формы имели бы смысл, в противном случае гораздо проще воспользоваться другим кодом:
```go
file, err := os.Open("file.csv")
if err != nil {
    // TODO: handle error
}
defer func(c io.Closer) {
    if cerr := c.Close(); cerr != nil {
        // TODO: handle error
    }
}(file)

reader := csv.NewReader(file)
// TODO: самостоятельно посмотрите реализацию ReadAll, увидите, что ошибка io.EOF там обрабатывается
records, err := reader.ReadAll()
if err != nil {
    // TODO: handle error	
}
```

Если у нас небольшие файлы*, то мы можем упростить код (передав задачи открытия и закрытия файла утилитным функциям):
```go
// data - []byte
data, err := ioutil.ReadFile("file.csv")
if err != nil {
    // TODO: handle error
}

reader := csv.NewReader(bytes.NewReader(data))
records, err := reader.ReadAll()
if err != nil {
    // TODO: handle error
}
```

Обратите внимание: для удобной работы со слайсами байтов у нас есть пакет `bytes` (так же как для строк был пакет `strings`). Именно он позволяет нам "завернуть" слайс байт в `bytes.Reader`, который удовлетворяет интерфейсу `io.Reader`.

Примечание*: мы уже несколько раз подряд повторили, что выбор способа реализации зависит от требований к задаче. Поэтому вы, как разработчик, всегда должны уточнять:
1. Какие файлы будут обрабатываться (минимальный, максимальный и средний размер - в Мб, строках, полях)
1. Известно ли, на какой машине планируется развёртывать ваше приложение (какие характеристики процессора, объём памяти и накопителя), либо вы сами должны предоставить требования
1. Сколько будет этих файлов, как часто они будут поступать (раз в день, раз в месяц или несколько раз в секунду)
1. Типичный сценарий использования вашего приложения

В нашем случае договоримся, что записей в импортируемом файле будет не более 1000.

### Что нужно сделать

Нужно сделать функцию импорта транзакций из файла формата CSV. Обратите внимание, что делая функцию импорта, вы столкнётесь со следующим вопросом: "где взять файл, который нужно импортировать"?

Варианта решения у вас два:
1. Написать экспорт
1. Написать файл вручную (либо используя Google Docs и другие инструменты)

Мы рекомендуем вам воспользоваться первым вариантом и гарантировать, что функции импорта могут импортировать то, что экспортировали функции экспорта (а то часто так бывает наоборот).

Итого: у вас должен быть проект, в котором в cmd будет:
1. `exporter` - пример с экспортом транзакций
1. `importer` - пример с импортом транзакций

Обратите внимание, что `csv.Reader` возвращает вам `[][]string`, т.е. вам придётся, используя функции из `strconv` преобразовывать данные из строчного типа в тип полей структуры (подумайте о том, чтобы вынести преобразование конкретной записи `[]string` в структуру транзакции в отдельную функцию, назовём её для примера `MapRowToTransaction`).

Не забывайте оформить всё так, как мы говорили на лекции:
1. Никаких `fmt.Println`
1. Логгирование
1. Функция `execute`, которая позволяет вам вернуть ненулевой код завершения процесса в случае ошибки

Итого, у вас должно быть: оформленный проект с пакетами и тестами, выложенный в репозиторий на GitHub и автотесты.

Тесты достаточно написать на функцию преобразования `MapRowToTransaction`.

## Задача №2 – JSON

Формат CSV достаточно часто используется для работы с "простыми" табличными данными.

**Q**: что значит "простыми"?

**A**: это значит у вас "плоский" набор полей. Т.е. каждое поле - это какое-то значение из серии число, строка, bool и т.д., а именно поле не хранит каких-то сложных структур, слайсов и т.д. (хотя это и возможно сделать).

Когда же нам нужно более сложное представление*, то используются другие форматы, например:
1. [JSON](https://tools.ietf.org/html/rfc7159)
1. [XML](https://www.w3.org/TR/REC-xml/)

Примечание*: либо более удобное для работы.

### JSON

JSON (JavaScript Object Notation) - легковесный текстовый формат, используется в качестве платформонезависимого формата для обмена данными между различными системами.

Содержит всего 6 типов данных:
1. Объект - набор пар ключ-значение
1. Массив - набор объектов (причём они не обязательно могут быть одного типа)
1. Строка
1. Число
1. Литералы `true` и `false`
1. Литерал `null` (важно: не `nil`, а `null`)

В стандартной библиотеке есть пакет json, который решает задачи кодирования в формат JSON и обратно:

```go
type Transaction struct {
	Id      int64
	From    string
	To      string
	Amount  int64
	Created int64
}

func main() {
	transaction := Transaction{
		Id:      1,
		From:    "0001",
		To:      "0002",
		Amount:  100_00,
		Created: time.Now().Unix(),
	}

	encoded, err := json.Marshal(transaction)
	if err != nil {
		// TODO: handle error
	}
	log.Println(string(encoded))

	var decoded Transaction
	// Важно: передаём указатель, чтобы функция смогла записать данные
	err = json.Unmarshal(encoded, &decoded)
	log.Printf("%#v", decoded)
    // если вдруг, у вас будет указатель:
    // var decoded *Transaction
    // то передавать придётся указатель на указатель
    // чтобы из функции можно было в исходный указатель записать данные:
	// err = json.Unmarshal(encoded, &decoded)
}
```

В выводе получим вот это: `{"Id":1,"From":"0001","To":"0002","Amount":10000,"Created":1593367588}`.

Это не очень хорошо, потому что в нормальных JSON-документах имена полей пишутся с маленькой буквы. Но если мы так сделаем, то получим неэкспортируемые поля и не сможем нормально работать с типом за пределами пакета.

Давайте попробуем это исправить. Пакеты xml и json в Go используют механизмы рефлексии для того, чтобы анализировать структуры, которые мы передаём. Рефлексия - это возможность программно анализировать код, в частности, разбирать, какие поля есть в структурах, какого они типа и т.д.

Чтобы добавлять какую-то мета-информацию, помогающую этим пакетам "настраивать" вывод, мы можем использовать теги структур.

Q: что это такое?

A: это просто запись формата: `Field Type "tag"`, которая потом анализируется пакетами xml и json.

В частности, для json мы можем написать вот так (в GoLand вы просто пишете json + TAB):

```go
type Transaction struct {
	Id      int64  `json:"id"`
	From    string `json:"from"`
	To      string `json:"to"`
	Amount  int64  `json:"amount"`
	Created int64  `json:"created"`
}
```

Теперь будет гораздо лучше:
```json
{"id":1,"from":"0001","to":"0002","amount":10000,"created":1593367904}
```

Причём обратное преобразование тоже работает без проблем. Полную информацию по тегам json вы можете найти [в документации](https://golang.org/pkg/encoding/json/#Marshal).

Тот же пример, но уже со слайсом:
```go
type Transaction struct {
	Id      int64  `json:"id"`
	From    string `json:"from"`
	To      string `json:"to"`
	Amount  int64  `json:"amount"`
	Created int64  `json:"created"`
}

func main() {
	transactions := []Transaction{
		{
			Id:      1,
			From:    "0001",
			To:      "0002",
			Amount:  100_00,
			Created: time.Now().Unix(),
		},
		{
			Id:      2,
			From:    "0001",
			To:      "0002",
			Amount:  200_00,
			Created: time.Now().Unix(),
		},
	}

	encoded, err := json.Marshal(transactions)
	if err != nil {
		// todo: handle error
	}
	log.Println(string(encoded))

	var decoded []Transaction
	// важно: передаём указатель
	err = json.Unmarshal(encoded, &decoded)
	log.Printf("%#v", decoded)
}
```

Как вы видите, JSON на порядок удобнее CSV, поскольку нам не приходится "вручную" конвертировать значения из строк. Кроме того, очень важно, что:
1. Он нативно (т.е. без дополнительных библиотек) поддерживается JS
1. Хорошо поддерживается в Android и iOS

Т.е. если мы будем в рамках построения backend'а отдавать клиентам JSON - они будут вполне счастливы.

### Reader & Writer

В более сложных сценариях, когда у вас есть ограничения по памяти, необходимость "тонкой настройки" и т.д., нужно использовать не `Marshal`/`Unmarshal`, а `Reader` и `Writer` из соответствующих пакетов.

### Что нужно сделать

Реализуйте экспорт и импорт данных о транзакциях не только в формате CSV, но и в формате JSON.

Оформите поддержку JSON в виде отдельных Pull Request'ов (их должно быть два). Автотесты писать не обязательно.
